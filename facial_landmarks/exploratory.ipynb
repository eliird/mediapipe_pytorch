{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facial_lm_model import FacialLM_Model\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from utils import *\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 192, 3)\n"
     ]
    }
   ],
   "source": [
    "weights = './model_weights/facial_landmarks.pth'\n",
    "model = FacialLM_Model()\n",
    "\n",
    "model.load_state_dict(torch.load(weights))\n",
    "model.eval()\n",
    "img_path = './4.jpg'\n",
    "img = cv2.imread(img_path).astype(np.float32)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = pad_image(img, desired_size=192)\n",
    "print(img.shape)\n",
    "landmarks = model.predict(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmotionDetector(nn.Module):\n",
    "    def __init__(self, landmark_model_weights, num_inp_imgs=10, num_labels=8):\n",
    "        super().__init__()\n",
    "        self.num_landmarks = 1404\n",
    "        self.feature_size = 32\n",
    "        \n",
    "        self.num_imgs = num_inp_imgs\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.landmark_model = FacialLM_Model()\n",
    "        self.landmark_model.load_state_dict(torch.load(landmark_model_weights))\n",
    "        \n",
    "        self.reducer = nn.Sequential(\n",
    "            nn.Linear(self.num_landmarks, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.feature_size),\n",
    "            nn.LayerNorm(self.feature_size),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.feature_size * self.num_imgs, num_labels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C, W, H = x.shape\n",
    "        x = x.view(B*N, C, W, H)\n",
    "        x, _ = self.landmark_model(x)\n",
    "        x = x.view(B * N, self.num_landmarks)\n",
    "        x = self.reducer(x)\n",
    "        x = x.view(B, self.num_imgs * self.feature_size)\n",
    "        x =  self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1197,  0.6324,  0.3485, -0.4941, -0.0358, -0.2341,  0.4940,  0.1616],\n",
       "        [-0.0140,  0.5639,  0.2450, -0.6044, -0.0431, -0.2267,  0.3871,  0.1949],\n",
       "        [-0.0202,  0.5701,  0.2593, -0.5343, -0.0249, -0.2307,  0.5697,  0.1302],\n",
       "        [-0.0023,  0.5904,  0.3055, -0.5262,  0.0047, -0.2352,  0.5406,  0.1575],\n",
       "        [-0.0131,  0.5361,  0.2388, -0.5475, -0.0085, -0.2263,  0.5451,  0.1488],\n",
       "        [-0.0242,  0.5695,  0.1122, -0.5907, -0.0551, -0.2630,  0.4002,  0.2077],\n",
       "        [-0.0501,  0.6384,  0.2886, -0.6630,  0.0265, -0.2439,  0.4587,  0.2685],\n",
       "        [ 0.0361,  0.5978,  0.2948, -0.4410,  0.0049, -0.2545,  0.5289,  0.1848]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_model = EmotionDetector('./model_weights/facial_landmarks.pth')\n",
    "video_model(torch.randn((8, 10, 3, 192, 192)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def process_video_all_frames(path: str, emotion: str, save_folder: str):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError(\"Invalid Video Path\")\n",
    "    \n",
    "    save_folder = os.path.join(save_folder, emotion)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    filename = path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    cap = cv2.VideoCapture(path)\n",
    "            \n",
    "    video = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret == False:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) \n",
    "        frame = pad_image(frame, desired_size=192)       \n",
    "        video.append(frame)\n",
    "        \n",
    "    save_path = os.path.join(save_folder, filename + '.npy') \n",
    "    np.save(save_path, np.array(video))\n",
    "    return\n",
    "\n",
    "test_video_path = \"/media/cv/Extreme Pro/MERR/mer2023train/train/sample_00002844.avi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>discrete</th>\n",
       "      <th>valence</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_00002721</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>/media/cv/Extreme Pro/MERR/mer2023train/train/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample_00005854</td>\n",
       "      <td>angry</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>/media/cv/Extreme Pro/MERR/mer2023train/train/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample_00003358</td>\n",
       "      <td>sad</td>\n",
       "      <td>-2.50</td>\n",
       "      <td>/media/cv/Extreme Pro/MERR/mer2023train/train/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample_00005933</td>\n",
       "      <td>happy</td>\n",
       "      <td>2.00</td>\n",
       "      <td>/media/cv/Extreme Pro/MERR/mer2023train/train/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample_00000613</td>\n",
       "      <td>angry</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>/media/cv/Extreme Pro/MERR/mer2023train/train/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name discrete  valence  \\\n",
       "0  sample_00002721  neutral    -0.25   \n",
       "1  sample_00005854    angry    -2.50   \n",
       "2  sample_00003358      sad    -2.50   \n",
       "3  sample_00005933    happy     2.00   \n",
       "4  sample_00000613    angry    -1.75   \n",
       "\n",
       "                                                path  \n",
       "0  /media/cv/Extreme Pro/MERR/mer2023train/train/...  \n",
       "1  /media/cv/Extreme Pro/MERR/mer2023train/train/...  \n",
       "2  /media/cv/Extreme Pro/MERR/mer2023train/train/...  \n",
       "3  /media/cv/Extreme Pro/MERR/mer2023train/train/...  \n",
       "4  /media/cv/Extreme Pro/MERR/mer2023train/train/...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "videos_path = \"/media/cv/Extreme Pro/MERR/mer2023train/train\"\n",
    "csv_path = \"/media/cv/Extreme Pro/MERR/mer2023train/train-label.csv\"\n",
    "\n",
    "def make_vid_path(filename, base_path):\n",
    "    return os.path.join(base_path, filename + '.avi')\n",
    "\n",
    "def load_csv(path, base_path):\n",
    "    df = pd.read_csv(path)\n",
    "    df['path'] = df['name'].apply(lambda x: make_vid_path(x, base_path))\n",
    "    return df\n",
    "\n",
    "df = load_csv(csv_path, videos_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 1901/3373 [00:00<00:00, 2743.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_00002946 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3373/3373 [04:30<00:00, 12.45it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "save_folder = './MER_Dataset/'\n",
    "\n",
    "failed_count = 0\n",
    "\n",
    "for row in tqdm(range(df.shape[0])):\n",
    "    if row < 1900:\n",
    "        continue\n",
    "    path = df.iloc[row]['path']\n",
    "    emotion = df.iloc[row]['discrete']\n",
    "    filename = df.iloc[row]['name']\n",
    "    try:\n",
    "        process_video_all_frames(path, emotion, save_folder)\n",
    "    except:\n",
    "        failed_count += 1\n",
    "        print(filename, failed_count)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksTemporal(Dataset):\n",
    "    def __init__(self, dataset_path: str, mode: str, num_imgs: int=10, split: float=0.75):\n",
    "        self._paths = self._load_paths(dataset_path, mode, split)\n",
    "        self.num_imgs = num_imgs\n",
    "        self.paths = []\n",
    "        for path in tqdm(self._paths):\n",
    "            try: \n",
    "                item = np.load(path)\n",
    "                self.paths.append(path)\n",
    "            except Exception as e:\n",
    "                print(\"nothing saved\", e)\n",
    "        random.shuffle(self.paths)\n",
    "        \n",
    "          \n",
    "        self.label2id = {'angry': 0, 'happy': 1, 'neutral': 2, 'sad': 3, 'surprise': 4, 'worried': 5}\n",
    "        self.id2label = {v:k for k, v in self.label2id.items()}\n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        item = torch.tensor(np.load(path), dtype=torch.float32)\n",
    "        FRAMES, C, W, H = item.shape\n",
    "        video = torch.zeros(self.num_imgs, C, W, H)\n",
    "        for i in range(min(FRAMES, self.num_imgs)):\n",
    "            video[i] = item[i]\n",
    "        video = video.permute(0, 3, 1, 2)\n",
    "\n",
    "        label = torch.tensor(self.label2id[path.split('/')[-2]])\n",
    "        return (video, label)        \n",
    "        \n",
    "    def _load_paths(self, base_folder: str, mode: str, split: float) -> list:\n",
    "        \n",
    "        emotions = [os.path.join(base_folder, emo) for emo in os.listdir(base_folder)]\n",
    "        paths = []\n",
    "        for emo in emotions:\n",
    "            files = os.listdir(emo)\n",
    "            if mode == 'train':\n",
    "                start = 0\n",
    "                end = int(len(files) * split)\n",
    "            elif mode == 'test':\n",
    "                start = int(len(files) * split)\n",
    "                end = len(files)\n",
    "            else:\n",
    "                raise ValueError(\"Sanity Check: No mode other than train and test is defined\")\n",
    "            files = files[start: end]\n",
    "            for file in files:\n",
    "                paths.append(os.path.join(emo, file))\n",
    "        # paths = [os.path.join(emo, file) for emo in emotions for file in os.listdir(emo)]\n",
    "        return paths\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_acc = []\n",
    "test_loss = [0]\n",
    "train_loss = [0]\n",
    "train_step = [0]\n",
    "\n",
    "\n",
    "def train(weights_save_path):\n",
    "    \n",
    "    '''\n",
    "    Updates the global variables need to check that\n",
    "    '''\n",
    "    \n",
    "    eval_loss = 0.0\n",
    "    eval_acc = 0.0\n",
    "    max_eval_acc = -1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for i, batch in enumerate(trainloader):\n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            landmarks, labels = batch\n",
    "            landmarks: torch.Tensor =landmarks.to(device)\n",
    "            labels: torch.Tensor = labels.to(device)\n",
    "            \n",
    "            logits = model(landmarks)\n",
    "            loss: torch.Tensor = criterion(logits, labels)\n",
    "            \n",
    "            \n",
    "            #back_prop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 25 == 0:\n",
    "                print(f\"Epoch: {epoch} | Loss: {loss.item():.4f} | Last Eval Loss: {eval_loss:.4f}| Last Eval Accuracy: {eval_acc:.4f}\")\n",
    "            \n",
    "            # track stats\n",
    "            train_loss.append(loss.item())\n",
    "            train_step.append(train_step[-1] + 1)\n",
    "        \n",
    "        # evaluate the model\n",
    "        eval_loss, eval_acc = test()\n",
    "        \n",
    "        # track evaluation dataset stats\n",
    "        test_acc.append(eval_acc)\n",
    "        test_loss.append(eval_loss)\n",
    "        \n",
    "        # save the best model\n",
    "        if eval_acc > max_eval_acc:\n",
    "            torch.save(model.state_dict(), weights_save_path)\n",
    "            max_eval_acc = eval_acc\n",
    "        \n",
    "    print(\"Evaluation Loss: \",  eval_loss, eval_acc)\n",
    "    \n",
    "    \n",
    "@torch.no_grad()         \n",
    "def test():\n",
    "    preds_ = []\n",
    "    labels_ = []\n",
    "    eval_loss = []\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(testloader):\n",
    "        \n",
    "        landmarks, labels = batch\n",
    "        landmarks = landmarks.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = model(landmarks)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        preds_.extend((preds.cpu().tolist()))\n",
    "        labels_.extend(labels.cpu().tolist())\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        eval_loss.append(loss.item())\n",
    "    \n",
    "    return (sum(eval_loss)/len(eval_loss), accuracy_score(preds_, labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2527 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 1739/2527 [00:03<00:01, 454.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing saved cannot reshape array of size 11947904 into shape (183,192,192,3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2527/2527 [00:05<00:00, 435.71it/s]\n",
      "100%|██████████| 846/846 [00:02<00:00, 419.21it/s]\n"
     ]
    }
   ],
   "source": [
    "device='cuda:1'\n",
    "epochs = 200\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "model = EmotionDetector('./model_weights/facial_landmarks.pth').to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# dataset = LandMarks(save_folder)\n",
    "# train, test = split_dataset(dataset, 0.75, random_seed=42)\n",
    "dataset_path = \"./MER_Dataset\"\n",
    "\n",
    "train_data = LandmarksTemporal(dataset_path, mode=\"train\")\n",
    "test_data = LandmarksTemporal(dataset_path, mode=\"test\")\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 2.4101 | Last Eval Loss: 0.0000| Last Eval Accuracy: 0.0000\n",
      "Epoch: 1 | Loss: 1.7262 | Last Eval Loss: 1.6941| Last Eval Accuracy: 0.2577\n",
      "Epoch: 2 | Loss: 1.6284 | Last Eval Loss: 1.6811| Last Eval Accuracy: 0.2577\n",
      "Epoch: 3 | Loss: 1.7144 | Last Eval Loss: 1.6853| Last Eval Accuracy: 0.2435\n",
      "Epoch: 4 | Loss: 1.7277 | Last Eval Loss: 1.6796| Last Eval Accuracy: 0.2577\n",
      "Epoch: 5 | Loss: 1.7428 | Last Eval Loss: 1.6968| Last Eval Accuracy: 0.2435\n",
      "Epoch: 6 | Loss: 1.6137 | Last Eval Loss: 1.6799| Last Eval Accuracy: 0.2435\n",
      "Epoch: 7 | Loss: 1.6628 | Last Eval Loss: 1.6784| Last Eval Accuracy: 0.2435\n",
      "Epoch: 8 | Loss: 1.5843 | Last Eval Loss: 1.6759| Last Eval Accuracy: 0.2435\n",
      "Epoch: 9 | Loss: 1.6496 | Last Eval Loss: 1.6769| Last Eval Accuracy: 0.2435\n",
      "Epoch: 10 | Loss: 1.6998 | Last Eval Loss: 1.6773| Last Eval Accuracy: 0.2577\n",
      "Epoch: 11 | Loss: 1.7181 | Last Eval Loss: 1.6741| Last Eval Accuracy: 0.2577\n",
      "Epoch: 12 | Loss: 1.6991 | Last Eval Loss: 1.6771| Last Eval Accuracy: 0.2435\n",
      "Epoch: 13 | Loss: 1.6791 | Last Eval Loss: 1.6734| Last Eval Accuracy: 0.2577\n",
      "Epoch: 14 | Loss: 1.7601 | Last Eval Loss: 1.6740| Last Eval Accuracy: 0.2577\n",
      "Epoch: 15 | Loss: 1.9049 | Last Eval Loss: 1.6764| Last Eval Accuracy: 0.2577\n",
      "Epoch: 16 | Loss: 1.6544 | Last Eval Loss: 1.6723| Last Eval Accuracy: 0.2577\n",
      "Epoch: 17 | Loss: 1.5998 | Last Eval Loss: 1.6715| Last Eval Accuracy: 0.2577\n",
      "Epoch: 18 | Loss: 1.6422 | Last Eval Loss: 1.6723| Last Eval Accuracy: 0.2577\n",
      "Epoch: 19 | Loss: 1.6327 | Last Eval Loss: 1.6780| Last Eval Accuracy: 0.2045\n",
      "Epoch: 20 | Loss: 1.6886 | Last Eval Loss: 1.6816| Last Eval Accuracy: 0.2577\n",
      "Epoch: 21 | Loss: 1.8038 | Last Eval Loss: 1.6737| Last Eval Accuracy: 0.2577\n",
      "Epoch: 22 | Loss: 1.5637 | Last Eval Loss: 1.6727| Last Eval Accuracy: 0.2435\n",
      "Epoch: 23 | Loss: 1.5053 | Last Eval Loss: 1.6759| Last Eval Accuracy: 0.2577\n",
      "Epoch: 24 | Loss: 1.6077 | Last Eval Loss: 1.6830| Last Eval Accuracy: 0.2435\n",
      "Epoch: 25 | Loss: 1.6291 | Last Eval Loss: 1.6739| Last Eval Accuracy: 0.2435\n",
      "Epoch: 26 | Loss: 1.6481 | Last Eval Loss: 1.6749| Last Eval Accuracy: 0.2577\n",
      "Epoch: 27 | Loss: 1.6361 | Last Eval Loss: 1.6739| Last Eval Accuracy: 0.2577\n",
      "Epoch: 28 | Loss: 1.6685 | Last Eval Loss: 1.6759| Last Eval Accuracy: 0.2577\n",
      "Epoch: 29 | Loss: 1.6484 | Last Eval Loss: 1.6678| Last Eval Accuracy: 0.2388\n",
      "Epoch: 30 | Loss: 1.6963 | Last Eval Loss: 1.6710| Last Eval Accuracy: 0.2577\n",
      "Epoch: 31 | Loss: 1.7595 | Last Eval Loss: 1.6730| Last Eval Accuracy: 0.2187\n",
      "Epoch: 32 | Loss: 1.6856 | Last Eval Loss: 1.6699| Last Eval Accuracy: 0.2577\n",
      "Epoch: 33 | Loss: 1.7496 | Last Eval Loss: 1.6683| Last Eval Accuracy: 0.2305\n",
      "Epoch: 34 | Loss: 1.6465 | Last Eval Loss: 1.6681| Last Eval Accuracy: 0.2553\n",
      "Epoch: 35 | Loss: 1.6866 | Last Eval Loss: 1.6671| Last Eval Accuracy: 0.2671\n",
      "Epoch: 36 | Loss: 1.6945 | Last Eval Loss: 1.6684| Last Eval Accuracy: 0.2530\n",
      "Epoch: 37 | Loss: 1.7576 | Last Eval Loss: 1.6795| Last Eval Accuracy: 0.2589\n",
      "Epoch: 38 | Loss: 1.7103 | Last Eval Loss: 1.6771| Last Eval Accuracy: 0.2707\n",
      "Epoch: 39 | Loss: 1.5702 | Last Eval Loss: 1.6719| Last Eval Accuracy: 0.2447\n",
      "Epoch: 40 | Loss: 1.6431 | Last Eval Loss: 1.6667| Last Eval Accuracy: 0.2400\n",
      "Epoch: 41 | Loss: 1.6269 | Last Eval Loss: 1.6700| Last Eval Accuracy: 0.2506\n",
      "Epoch: 42 | Loss: 1.5375 | Last Eval Loss: 1.6690| Last Eval Accuracy: 0.2364\n",
      "Epoch: 43 | Loss: 1.6810 | Last Eval Loss: 1.6686| Last Eval Accuracy: 0.2624\n",
      "Epoch: 44 | Loss: 1.8346 | Last Eval Loss: 1.6696| Last Eval Accuracy: 0.2624\n",
      "Epoch: 45 | Loss: 1.6658 | Last Eval Loss: 1.6674| Last Eval Accuracy: 0.2577\n",
      "Epoch: 46 | Loss: 1.6364 | Last Eval Loss: 1.6729| Last Eval Accuracy: 0.2411\n",
      "Epoch: 47 | Loss: 1.6133 | Last Eval Loss: 1.6706| Last Eval Accuracy: 0.2648\n",
      "Epoch: 48 | Loss: 1.6557 | Last Eval Loss: 1.6689| Last Eval Accuracy: 0.2518\n",
      "Epoch: 49 | Loss: 1.7224 | Last Eval Loss: 1.6720| Last Eval Accuracy: 0.2340\n",
      "Epoch: 50 | Loss: 1.6793 | Last Eval Loss: 1.6706| Last Eval Accuracy: 0.2648\n",
      "Epoch: 51 | Loss: 1.7551 | Last Eval Loss: 1.6701| Last Eval Accuracy: 0.2530\n",
      "Epoch: 52 | Loss: 1.6071 | Last Eval Loss: 1.6739| Last Eval Accuracy: 0.2281\n",
      "Epoch: 53 | Loss: 1.8216 | Last Eval Loss: 1.6696| Last Eval Accuracy: 0.2719\n",
      "Epoch: 54 | Loss: 1.5851 | Last Eval Loss: 1.6713| Last Eval Accuracy: 0.2400\n",
      "Epoch: 55 | Loss: 1.6102 | Last Eval Loss: 1.6705| Last Eval Accuracy: 0.2600\n",
      "Epoch: 56 | Loss: 1.6532 | Last Eval Loss: 1.6708| Last Eval Accuracy: 0.2506\n",
      "Epoch: 57 | Loss: 1.7353 | Last Eval Loss: 1.6809| Last Eval Accuracy: 0.2258\n",
      "Epoch: 58 | Loss: 1.6990 | Last Eval Loss: 1.6732| Last Eval Accuracy: 0.2435\n",
      "Epoch: 59 | Loss: 1.4978 | Last Eval Loss: 1.6694| Last Eval Accuracy: 0.2541\n",
      "Epoch: 60 | Loss: 1.6351 | Last Eval Loss: 1.6839| Last Eval Accuracy: 0.2506\n",
      "Epoch: 61 | Loss: 1.5083 | Last Eval Loss: 1.6705| Last Eval Accuracy: 0.2589\n",
      "Epoch: 62 | Loss: 1.7168 | Last Eval Loss: 1.6687| Last Eval Accuracy: 0.2506\n",
      "Epoch: 63 | Loss: 1.5781 | Last Eval Loss: 1.6748| Last Eval Accuracy: 0.2340\n",
      "Epoch: 64 | Loss: 1.6384 | Last Eval Loss: 1.6751| Last Eval Accuracy: 0.2423\n",
      "Epoch: 65 | Loss: 1.5941 | Last Eval Loss: 1.6790| Last Eval Accuracy: 0.2470\n",
      "Epoch: 66 | Loss: 1.7631 | Last Eval Loss: 1.6776| Last Eval Accuracy: 0.2376\n",
      "Epoch: 67 | Loss: 1.7469 | Last Eval Loss: 1.6771| Last Eval Accuracy: 0.2577\n",
      "Epoch: 68 | Loss: 1.6475 | Last Eval Loss: 1.6765| Last Eval Accuracy: 0.2482\n",
      "Epoch: 69 | Loss: 1.4974 | Last Eval Loss: 1.6810| Last Eval Accuracy: 0.2577\n",
      "Epoch: 70 | Loss: 1.6827 | Last Eval Loss: 1.6845| Last Eval Accuracy: 0.2400\n",
      "Epoch: 71 | Loss: 1.5982 | Last Eval Loss: 1.6769| Last Eval Accuracy: 0.2589\n",
      "Epoch: 72 | Loss: 1.6137 | Last Eval Loss: 1.6770| Last Eval Accuracy: 0.2234\n",
      "Epoch: 73 | Loss: 1.7294 | Last Eval Loss: 1.6812| Last Eval Accuracy: 0.2470\n",
      "Epoch: 74 | Loss: 1.7002 | Last Eval Loss: 1.6800| Last Eval Accuracy: 0.2577\n",
      "Epoch: 75 | Loss: 1.6030 | Last Eval Loss: 1.6847| Last Eval Accuracy: 0.2541\n",
      "Epoch: 76 | Loss: 1.5964 | Last Eval Loss: 1.6797| Last Eval Accuracy: 0.2340\n",
      "Epoch: 77 | Loss: 1.5598 | Last Eval Loss: 1.6864| Last Eval Accuracy: 0.2648\n",
      "Epoch: 78 | Loss: 1.4254 | Last Eval Loss: 1.6820| Last Eval Accuracy: 0.2482\n",
      "Epoch: 79 | Loss: 1.7239 | Last Eval Loss: 1.6719| Last Eval Accuracy: 0.2518\n",
      "Epoch: 80 | Loss: 1.6146 | Last Eval Loss: 1.6805| Last Eval Accuracy: 0.2329\n",
      "Epoch: 81 | Loss: 1.6935 | Last Eval Loss: 1.6761| Last Eval Accuracy: 0.2624\n",
      "Epoch: 82 | Loss: 1.5861 | Last Eval Loss: 1.6789| Last Eval Accuracy: 0.2624\n",
      "Epoch: 83 | Loss: 1.5934 | Last Eval Loss: 1.6815| Last Eval Accuracy: 0.2683\n",
      "Epoch: 84 | Loss: 1.7194 | Last Eval Loss: 1.6841| Last Eval Accuracy: 0.2305\n",
      "Epoch: 85 | Loss: 1.6430 | Last Eval Loss: 1.6874| Last Eval Accuracy: 0.2648\n",
      "Epoch: 86 | Loss: 1.6408 | Last Eval Loss: 1.6866| Last Eval Accuracy: 0.2778\n",
      "Epoch: 87 | Loss: 1.6565 | Last Eval Loss: 1.6919| Last Eval Accuracy: 0.2648\n",
      "Epoch: 88 | Loss: 1.6140 | Last Eval Loss: 1.6953| Last Eval Accuracy: 0.2541\n",
      "Epoch: 89 | Loss: 1.7389 | Last Eval Loss: 1.6752| Last Eval Accuracy: 0.2553\n",
      "Epoch: 90 | Loss: 1.5945 | Last Eval Loss: 1.6875| Last Eval Accuracy: 0.2459\n",
      "Epoch: 91 | Loss: 1.7472 | Last Eval Loss: 1.6925| Last Eval Accuracy: 0.2435\n",
      "Epoch: 92 | Loss: 1.6739 | Last Eval Loss: 1.6857| Last Eval Accuracy: 0.2518\n",
      "Epoch: 93 | Loss: 1.5745 | Last Eval Loss: 1.6906| Last Eval Accuracy: 0.2352\n",
      "Epoch: 94 | Loss: 1.5454 | Last Eval Loss: 1.6894| Last Eval Accuracy: 0.2695\n",
      "Epoch: 95 | Loss: 1.6836 | Last Eval Loss: 1.6877| Last Eval Accuracy: 0.2411\n",
      "Epoch: 96 | Loss: 1.6616 | Last Eval Loss: 1.6850| Last Eval Accuracy: 0.2660\n",
      "Epoch: 97 | Loss: 1.6415 | Last Eval Loss: 1.6873| Last Eval Accuracy: 0.2530\n",
      "Epoch: 98 | Loss: 1.6438 | Last Eval Loss: 1.6921| Last Eval Accuracy: 0.2518\n",
      "Epoch: 99 | Loss: 1.5380 | Last Eval Loss: 1.6903| Last Eval Accuracy: 0.2494\n",
      "Epoch: 100 | Loss: 1.5672 | Last Eval Loss: 1.6880| Last Eval Accuracy: 0.2624\n",
      "Epoch: 101 | Loss: 1.6077 | Last Eval Loss: 1.6822| Last Eval Accuracy: 0.2518\n",
      "Epoch: 102 | Loss: 1.6697 | Last Eval Loss: 1.7003| Last Eval Accuracy: 0.2530\n",
      "Epoch: 103 | Loss: 1.5672 | Last Eval Loss: 1.7018| Last Eval Accuracy: 0.2541\n",
      "Epoch: 104 | Loss: 1.5869 | Last Eval Loss: 1.6880| Last Eval Accuracy: 0.2270\n",
      "Epoch: 105 | Loss: 1.6211 | Last Eval Loss: 1.6940| Last Eval Accuracy: 0.2423\n",
      "Epoch: 106 | Loss: 1.7341 | Last Eval Loss: 1.7035| Last Eval Accuracy: 0.2376\n",
      "Epoch: 107 | Loss: 1.5311 | Last Eval Loss: 1.6978| Last Eval Accuracy: 0.2340\n",
      "Epoch: 108 | Loss: 1.7194 | Last Eval Loss: 1.6978| Last Eval Accuracy: 0.2565\n",
      "Epoch: 109 | Loss: 1.5811 | Last Eval Loss: 1.6934| Last Eval Accuracy: 0.2600\n",
      "Epoch: 110 | Loss: 1.3898 | Last Eval Loss: 1.7030| Last Eval Accuracy: 0.2329\n",
      "Epoch: 111 | Loss: 1.5514 | Last Eval Loss: 1.7154| Last Eval Accuracy: 0.2518\n",
      "Epoch: 112 | Loss: 1.6985 | Last Eval Loss: 1.6960| Last Eval Accuracy: 0.2400\n",
      "Epoch: 113 | Loss: 1.5820 | Last Eval Loss: 1.7011| Last Eval Accuracy: 0.2400\n",
      "Epoch: 114 | Loss: 1.5461 | Last Eval Loss: 1.7170| Last Eval Accuracy: 0.2482\n",
      "Epoch: 115 | Loss: 1.6518 | Last Eval Loss: 1.7074| Last Eval Accuracy: 0.2518\n",
      "Epoch: 116 | Loss: 1.9326 | Last Eval Loss: 1.7165| Last Eval Accuracy: 0.2459\n",
      "Epoch: 117 | Loss: 1.7358 | Last Eval Loss: 1.7139| Last Eval Accuracy: 0.2305\n",
      "Epoch: 118 | Loss: 1.5110 | Last Eval Loss: 1.7207| Last Eval Accuracy: 0.2246\n",
      "Epoch: 119 | Loss: 1.5188 | Last Eval Loss: 1.7196| Last Eval Accuracy: 0.2352\n",
      "Epoch: 120 | Loss: 1.5085 | Last Eval Loss: 1.7075| Last Eval Accuracy: 0.2506\n",
      "Epoch: 121 | Loss: 1.5259 | Last Eval Loss: 1.7245| Last Eval Accuracy: 0.2293\n",
      "Epoch: 122 | Loss: 1.5969 | Last Eval Loss: 1.7213| Last Eval Accuracy: 0.2530\n",
      "Epoch: 123 | Loss: 1.5452 | Last Eval Loss: 1.7326| Last Eval Accuracy: 0.2400\n",
      "Epoch: 124 | Loss: 1.5118 | Last Eval Loss: 1.7345| Last Eval Accuracy: 0.2246\n",
      "Epoch: 125 | Loss: 1.4292 | Last Eval Loss: 1.7200| Last Eval Accuracy: 0.2506\n",
      "Epoch: 126 | Loss: 1.4376 | Last Eval Loss: 1.7263| Last Eval Accuracy: 0.2388\n",
      "Epoch: 127 | Loss: 1.6324 | Last Eval Loss: 1.7164| Last Eval Accuracy: 0.2364\n",
      "Epoch: 128 | Loss: 1.8623 | Last Eval Loss: 1.7258| Last Eval Accuracy: 0.2376\n",
      "Epoch: 129 | Loss: 1.5488 | Last Eval Loss: 1.7194| Last Eval Accuracy: 0.2376\n",
      "Epoch: 130 | Loss: 1.6222 | Last Eval Loss: 1.7364| Last Eval Accuracy: 0.2470\n",
      "Epoch: 131 | Loss: 1.4233 | Last Eval Loss: 1.7417| Last Eval Accuracy: 0.2258\n",
      "Epoch: 132 | Loss: 1.5939 | Last Eval Loss: 1.7374| Last Eval Accuracy: 0.2447\n",
      "Epoch: 133 | Loss: 1.6417 | Last Eval Loss: 1.7650| Last Eval Accuracy: 0.2258\n",
      "Epoch: 134 | Loss: 1.5964 | Last Eval Loss: 1.7435| Last Eval Accuracy: 0.2329\n",
      "Epoch: 135 | Loss: 1.7369 | Last Eval Loss: 1.7531| Last Eval Accuracy: 0.2128\n",
      "Epoch: 136 | Loss: 1.7281 | Last Eval Loss: 1.7372| Last Eval Accuracy: 0.2305\n",
      "Epoch: 137 | Loss: 1.6376 | Last Eval Loss: 1.7504| Last Eval Accuracy: 0.2199\n",
      "Epoch: 138 | Loss: 1.6189 | Last Eval Loss: 1.7642| Last Eval Accuracy: 0.2376\n",
      "Epoch: 139 | Loss: 1.4952 | Last Eval Loss: 1.7463| Last Eval Accuracy: 0.2199\n",
      "Epoch: 140 | Loss: 1.4823 | Last Eval Loss: 1.7431| Last Eval Accuracy: 0.2281\n",
      "Epoch: 141 | Loss: 1.6239 | Last Eval Loss: 1.7506| Last Eval Accuracy: 0.2305\n",
      "Epoch: 142 | Loss: 1.6369 | Last Eval Loss: 1.7550| Last Eval Accuracy: 0.2270\n",
      "Epoch: 143 | Loss: 1.5113 | Last Eval Loss: 1.7647| Last Eval Accuracy: 0.2388\n",
      "Epoch: 144 | Loss: 1.4432 | Last Eval Loss: 1.7522| Last Eval Accuracy: 0.2151\n",
      "Epoch: 145 | Loss: 1.6627 | Last Eval Loss: 1.7673| Last Eval Accuracy: 0.2151\n",
      "Epoch: 146 | Loss: 1.7062 | Last Eval Loss: 1.7775| Last Eval Accuracy: 0.2329\n",
      "Epoch: 147 | Loss: 1.5104 | Last Eval Loss: 1.7901| Last Eval Accuracy: 0.2281\n",
      "Epoch: 148 | Loss: 1.7016 | Last Eval Loss: 1.7642| Last Eval Accuracy: 0.1962\n",
      "Epoch: 149 | Loss: 1.5223 | Last Eval Loss: 1.7715| Last Eval Accuracy: 0.2092\n",
      "Epoch: 150 | Loss: 1.6081 | Last Eval Loss: 1.7797| Last Eval Accuracy: 0.2069\n",
      "Epoch: 151 | Loss: 1.6574 | Last Eval Loss: 1.7696| Last Eval Accuracy: 0.2317\n",
      "Epoch: 152 | Loss: 1.5345 | Last Eval Loss: 1.7714| Last Eval Accuracy: 0.2199\n",
      "Epoch: 153 | Loss: 1.5627 | Last Eval Loss: 1.7750| Last Eval Accuracy: 0.2139\n",
      "Epoch: 154 | Loss: 1.5457 | Last Eval Loss: 1.7636| Last Eval Accuracy: 0.2069\n",
      "Epoch: 155 | Loss: 1.6298 | Last Eval Loss: 1.7885| Last Eval Accuracy: 0.2092\n",
      "Epoch: 156 | Loss: 1.4848 | Last Eval Loss: 1.7749| Last Eval Accuracy: 0.2116\n",
      "Epoch: 157 | Loss: 1.5603 | Last Eval Loss: 1.7807| Last Eval Accuracy: 0.2222\n",
      "Epoch: 158 | Loss: 1.6190 | Last Eval Loss: 1.7796| Last Eval Accuracy: 0.2151\n",
      "Epoch: 159 | Loss: 1.4947 | Last Eval Loss: 1.7693| Last Eval Accuracy: 0.2175\n",
      "Epoch: 160 | Loss: 1.5124 | Last Eval Loss: 1.7696| Last Eval Accuracy: 0.2364\n",
      "Epoch: 161 | Loss: 1.5864 | Last Eval Loss: 1.7865| Last Eval Accuracy: 0.2116\n",
      "Epoch: 162 | Loss: 1.4165 | Last Eval Loss: 1.7808| Last Eval Accuracy: 0.2281\n",
      "Epoch: 163 | Loss: 1.6177 | Last Eval Loss: 1.7787| Last Eval Accuracy: 0.2210\n",
      "Epoch: 164 | Loss: 1.5820 | Last Eval Loss: 1.7789| Last Eval Accuracy: 0.2080\n",
      "Epoch: 165 | Loss: 1.5464 | Last Eval Loss: 1.7936| Last Eval Accuracy: 0.2246\n",
      "Epoch: 166 | Loss: 1.5260 | Last Eval Loss: 1.7719| Last Eval Accuracy: 0.2128\n",
      "Epoch: 167 | Loss: 1.7302 | Last Eval Loss: 1.8008| Last Eval Accuracy: 0.2092\n",
      "Epoch: 168 | Loss: 1.4688 | Last Eval Loss: 1.7835| Last Eval Accuracy: 0.2104\n",
      "Epoch: 169 | Loss: 1.5394 | Last Eval Loss: 1.8051| Last Eval Accuracy: 0.2116\n",
      "Epoch: 170 | Loss: 1.4755 | Last Eval Loss: 1.8046| Last Eval Accuracy: 0.2175\n",
      "Epoch: 171 | Loss: 1.6865 | Last Eval Loss: 1.7968| Last Eval Accuracy: 0.2293\n",
      "Epoch: 172 | Loss: 1.5778 | Last Eval Loss: 1.8035| Last Eval Accuracy: 0.2139\n",
      "Epoch: 173 | Loss: 1.4330 | Last Eval Loss: 1.7957| Last Eval Accuracy: 0.2199\n",
      "Epoch: 174 | Loss: 1.4679 | Last Eval Loss: 1.8220| Last Eval Accuracy: 0.2317\n",
      "Epoch: 175 | Loss: 1.6264 | Last Eval Loss: 1.8180| Last Eval Accuracy: 0.2163\n",
      "Epoch: 176 | Loss: 1.7237 | Last Eval Loss: 1.8040| Last Eval Accuracy: 0.2270\n",
      "Epoch: 177 | Loss: 1.5316 | Last Eval Loss: 1.8082| Last Eval Accuracy: 0.2175\n",
      "Epoch: 178 | Loss: 1.5222 | Last Eval Loss: 1.8239| Last Eval Accuracy: 0.2139\n",
      "Epoch: 179 | Loss: 1.4434 | Last Eval Loss: 1.8175| Last Eval Accuracy: 0.2151\n",
      "Epoch: 180 | Loss: 1.6474 | Last Eval Loss: 1.8007| Last Eval Accuracy: 0.2293\n",
      "Epoch: 181 | Loss: 1.5400 | Last Eval Loss: 1.8398| Last Eval Accuracy: 0.2246\n",
      "Epoch: 182 | Loss: 1.2192 | Last Eval Loss: 1.8175| Last Eval Accuracy: 0.2128\n",
      "Epoch: 183 | Loss: 1.6114 | Last Eval Loss: 1.8145| Last Eval Accuracy: 0.1986\n",
      "Epoch: 184 | Loss: 1.2864 | Last Eval Loss: 1.8249| Last Eval Accuracy: 0.2199\n",
      "Epoch: 185 | Loss: 1.4785 | Last Eval Loss: 1.8342| Last Eval Accuracy: 0.2376\n",
      "Epoch: 186 | Loss: 1.5842 | Last Eval Loss: 1.8169| Last Eval Accuracy: 0.2364\n",
      "Epoch: 187 | Loss: 1.4855 | Last Eval Loss: 1.8280| Last Eval Accuracy: 0.2104\n",
      "Epoch: 188 | Loss: 1.5834 | Last Eval Loss: 1.8370| Last Eval Accuracy: 0.2210\n",
      "Epoch: 189 | Loss: 1.5041 | Last Eval Loss: 1.8313| Last Eval Accuracy: 0.2080\n",
      "Epoch: 190 | Loss: 1.6046 | Last Eval Loss: 1.8233| Last Eval Accuracy: 0.2234\n",
      "Epoch: 191 | Loss: 1.5315 | Last Eval Loss: 1.8282| Last Eval Accuracy: 0.2187\n",
      "Epoch: 192 | Loss: 1.8213 | Last Eval Loss: 1.8386| Last Eval Accuracy: 0.2234\n",
      "Epoch: 193 | Loss: 1.4594 | Last Eval Loss: 1.8340| Last Eval Accuracy: 0.2080\n",
      "Epoch: 194 | Loss: 1.3839 | Last Eval Loss: 1.8427| Last Eval Accuracy: 0.2199\n",
      "Epoch: 195 | Loss: 1.4970 | Last Eval Loss: 1.8452| Last Eval Accuracy: 0.2246\n",
      "Epoch: 196 | Loss: 1.5036 | Last Eval Loss: 1.8722| Last Eval Accuracy: 0.2270\n",
      "Epoch: 197 | Loss: 1.4489 | Last Eval Loss: 1.8646| Last Eval Accuracy: 0.2175\n",
      "Epoch: 198 | Loss: 1.4503 | Last Eval Loss: 1.8471| Last Eval Accuracy: 0.2116\n",
      "Epoch: 199 | Loss: 1.5419 | Last Eval Loss: 1.8541| Last Eval Accuracy: 0.2163\n",
      "Evaluation Loss:  1.852130201127794 0.22458628841607564\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "test_loss = [0]\n",
    "train_loss = [0]\n",
    "train_step = [0]\n",
    "\n",
    "train('./model_weights/temporal_landmark.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tte",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
